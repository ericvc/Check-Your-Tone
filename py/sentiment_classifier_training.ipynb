{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_text_sentiment_classifiers.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "b5axepN8kvsn"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBtm6y0b1VCb"
      },
      "source": [
        "# **Check Your Tone: A Speech-To-Text Sentiment Analyzer for Raspberry Pi**\n",
        "This Google Colab notebook is used to train neural network models for text sentiment classification, as part of the Check Your Tone speech-to-text sentiment classifier project for Raspberry Pi. [See my GitHub repo](https://github.com/ericvc/Check-Your-Tone) for more information about the project and additional code necessary to get it up and running.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM1wMf17HTjP"
      },
      "source": [
        "## **0) Prepare Workspace for Model Fitting**\n",
        "\n",
        "### **Hardware Acceleration**\n",
        "\n",
        "Check Hardware Acceleration settings and verify GPU type. If hardware acceleration is not enabled, go to **Runtime > Change Runtime Type** and select GPU from the Hardware Accelerator dropdown menu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_fzzbQGHUYV"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqMw0M28RcSO"
      },
      "source": [
        "### **Connect Colaboratory Runtime to Google Drive to Save Files**\n",
        "\n",
        "Connecting to Google Drive will allow you to save workspace files for use in later runs and to save fitted models for download to your local "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WpqddLb0z-_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "if not os.path.exists(\"/content/drive/CYT\"):\n",
        "  !mkdir \"/content/drive/My Drive/CYT/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-82DiJf1LA1"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Load Python modules, define helper functions, and prepare data**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Load Python modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZtP94vT07HR"
      },
      "source": [
        "!pip install tf-nightly\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import L1L2\n",
        "from keras.utils import plot_model\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rgpMJe72V6v"
      },
      "source": [
        "Download the IMDB movie reviews dataset from [ai.stanford.edu/%7Eamaas/data/sentiment/aclImdb_v1.tar.gz](https://ai.stanford.edu/%7Eamaas/data/sentiment/aclImdb_v1.tar.gz)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1ke8D932fSc"
      },
      "source": [
        "%%capture\n",
        "#Download file with wget\n",
        "!wget ai.stanford.edu/%7Eamaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "#Extract from compressed file\n",
        "!tar xvzf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R_IGoM-zDkn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Define functions to process raw text for tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrbK4WJ01x7-"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\"\"\"\n",
        "Functions for processing raw text. Removes or replaces special characters, empty spaces, etc.\n",
        "\"\"\"\n",
        "\n",
        "stop_words = stopwords.words(\"english\")\n",
        "def REPLACE_STOP_WORDS_NO_SPACE(x):\n",
        "    # list comprehension to split input string into list of words, then evaluate each word\n",
        "    words = [word for word in x.split() if word not in stop_words]\n",
        "    # recombine the list of remaining words into a string\n",
        "    words_no_stop = \" \".join(words)\n",
        "    return words_no_stop\n",
        "\n",
        "\n",
        "def REPLACE_ELLIPSES_WITH_SPACE(x):\n",
        "    return re.compile(\"\\\\.{2,}\").sub(\" \", x)\n",
        "\n",
        "\n",
        "def REPLACE_CHARACTER_NO_SPACE(x):\n",
        "    return re.compile(\"[\\\\.\\\\-;:!\\'?,\\\"()\\[\\]\\/]\").sub(\"\", x)\n",
        "\n",
        "\n",
        "def REPLACE_BLANK_START_NO_SPACE(x):\n",
        "    return re.compile(\"^\\\\s+\").sub(\"\", x)\n",
        "\n",
        "\n",
        "def REPLACE_BLANK_END_NO_SPACE(x):\n",
        "    return re.compile(\"\\\\s+$\").sub(\"\", x)\n",
        "\n",
        "\n",
        "def REPLACE_BLANK_WITH_SPACE(x):\n",
        "    return re.compile(\"\\\\s{2,}\").sub(\" \", x)\n",
        "\n",
        "\n",
        "def REPLACE_FORMAT_NO_SPACE(x):\n",
        "    return re.compile(\"&\\\\w\").sub(\" \", x)\n",
        "\n",
        "\n",
        "def pre_process_sentence(sentences):\n",
        "    sentences = [REPLACE_ELLIPSES_WITH_SPACE(line) for line in sentences]\n",
        "    sentences = [REPLACE_CHARACTER_NO_SPACE(line) for line in sentences]\n",
        "    sentences = [REPLACE_FORMAT_NO_SPACE(line) for line in sentences]\n",
        "    sentences = [REPLACE_BLANK_START_NO_SPACE(line) for line in sentences]\n",
        "    sentences = [REPLACE_BLANK_END_NO_SPACE(line) for line in sentences]\n",
        "    sentences = [REPLACE_BLANK_WITH_SPACE(line) for line in sentences]\n",
        "    sentences = [REPLACE_STOP_WORDS_NO_SPACE(line) for line in sentences]\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVKbgIzd3Y9R"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "With the raw data extracted, we can get to work preparing the training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O1FN0FL3ffT"
      },
      "source": [
        "# Function for extracting text from each review file\n",
        "def get_review_text(dir):\n",
        "    files = os.listdir(dir)\n",
        "    reviews = []\n",
        "    for file in files:\n",
        "        file_path = dir + file\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            review = f.read().decode(\"utf-8\")\n",
        "        reviews.append(review)\n",
        "    return reviews\n",
        "\n",
        "## Get raw text of positive reviews\n",
        "pos_dir1 = \"aclImdb/train/pos/\"\n",
        "pos_dir2 = \"aclImdb/test/pos/\"\n",
        "positive_reviews = [*get_review_text(pos_dir1), *get_review_text(pos_dir2)]\n",
        "\n",
        "## Get raw text of negative reviews\n",
        "neg_dir1 = \"aclImdb/train/neg/\"\n",
        "neg_dir2 = \"aclImdb/test/neg/\"\n",
        "negative_reviews = [*get_review_text(neg_dir1), *get_review_text(neg_dir2)]\n",
        "\n",
        "## Combine all reviews into a pandas DataFrame\n",
        "all_reviews = [*positive_reviews, *negative_reviews]\n",
        "df = pd.DataFrame()\n",
        "df[\"sentence\"] = all_reviews  # review text as column 'sentence'\n",
        "df['sentence.lower'] = [text.lower() for text in df['sentence']]  # convert to lowercase letters\n",
        "labels = np.zeros(len(all_reviews))\n",
        "labels[0:int(len(all_reviews) / 2)] = 1\n",
        "df[\"label\"] = labels  # label value as column 'label'\n",
        "\n",
        "## Write processed data to CSV file\n",
        "df.to_csv(\"imdb_reviews_labeled.csv\")\n",
        "\n",
        "## Shuffle rows to mix positive and negative reviews\n",
        "df = pd.read_csv(\"imdb_reviews_labeled.csv\").sample(frac=1, random_state=222)\n",
        "sentences = df['sentence.lower'].values\n",
        "y = df['label'].values\n",
        "\n",
        "print(\"Data saved to CSV file.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1mcOeZ540FQ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Download pre-trained word embeddings from the GloVe project: https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "I will be using the embedding trained on the Common Crawl with 42B tokens and 300 dimension vectors (**~1.75 GB download**).\n",
        "\n",
        "On the first run of this notebook, the embeddings will be downloaded and saved to my Google Drive project folder. On subsequent runs, the saved version will be copied from Drive project folder to the Colab workspace, rather than downloaded again from the source. *This saves around 10-15 minutes per run*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_tjIAWi4zuJ"
      },
      "source": [
        "if not os.path.exists(\"/content/drive/My Drive/CYT/glove.42B.300d.zip\"):\n",
        "  !wget http://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip\n",
        "  !gsutil cp \"glove.42B.300d.zip\" \"/content/drive/My Drive/CYT/\"\n",
        "  !unzip \"glove.42B.300d.zip\"\n",
        "\n",
        "else:\n",
        "  !gsutil cp \"/content/drive/My Drive/CYT/glove.42B.300d.zip\" \"/content/\"\n",
        "  print(\"File copied to local workspace. Unzipping archive.\")\n",
        "  !unzip \"glove.42B.300d.zip\"\n",
        "\n",
        "print(\"Archive downloaded and inflated.\")\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLvj0Wgl5kuf"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "### **Tokenize IMDB Review Text Using Pre-Trained Embedding**\n",
        "Tokenize and prepare the embedding matrix used to train the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfKZzCgF5og2"
      },
      "source": [
        "## Clean review text\n",
        "X_processed = pre_process_sentence(sentences)\n",
        "\n",
        "## Tokenize words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_processed)\n",
        "X_tokenized = tokenizer.texts_to_sequences(X_processed)\n",
        "\n",
        "## Save fitted tokenizer to storage for use later on\n",
        "if not os.path.exists(\"tokenizer\"):\n",
        "  os.mkdir(\"tokenizer\")\n",
        "filename = 'tokenizer/sentence_tokenizer_fitted.sav'\n",
        "pickle.dump(tokenizer, open(filename, 'wb'))\n",
        "\n",
        "## Dimensions for word embeddings\n",
        "maxlen = 250\n",
        "embedding_dim = 300\n",
        "\n",
        "## Pad sequences (right side only) with 0s\n",
        "X = pad_sequences(X_tokenized, padding='post', maxlen=maxlen)\n",
        "\n",
        "## Create embedding matrix\n",
        "# define function for extracting word embeddings (line-by-line) from file\n",
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    with open(filepath, encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word]\n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "## Create embedding matrix\n",
        "embedding_matrix = create_embedding_matrix(\n",
        "    'glove.42B.300d.txt',\n",
        "    tokenizer.word_index, embedding_dim)\n",
        "\n",
        "## Check proportion of words included in pre-trained word embeddings.\n",
        "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(f\"{int(np.round(nonzero_elements / vocab_size, 2) * 100)}% of words are included in the pre-trained embedding.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5I80GszOPaB"
      },
      "source": [
        "---\n",
        "### **Define functions for specifying neural network models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTsfga81-igb"
      },
      "source": [
        "## Model template functions\n",
        "# RNN\n",
        "def create_lstm_model(learn_rate: float = 0.001, units: int = 32, n_blocks: int=0):\n",
        "    # Define optimization settings\n",
        "    optimizer = Adam(lr=learn_rate)\n",
        "\n",
        "    # Initialize model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add embedding layer\n",
        "    model.add(layers.Embedding(vocab_size, embedding_dim,\n",
        "                               weights=[embedding_matrix],\n",
        "                               input_length=maxlen,\n",
        "                               trainable=False, \n",
        "                               name=\"embedding\"))\n",
        "\n",
        "    # Add LSTM blocks\n",
        "    for blocks in range(n_blocks):\n",
        "        label = str(blocks)\n",
        "        model.add(layers.LSTM(units=units, name=\"lstm_\"+label, \n",
        "                              return_sequences=True, \n",
        "                              activity_regularizer=L1L2(0.0, 1e-2)))\n",
        "        model.add(layers.BatchNormalization(name=\"batch_norm_\"+label))\n",
        "        model.add(layers.Activation(\"elu\", name=\"activation_\"+label))\n",
        "\n",
        "    model.add(layers.LSTM(units=units, name=\"lstm_final\", \n",
        "                          activity_regularizer=L1L2(0.0, 1e-2)))\n",
        "    model.add(layers.BatchNormalization(name=\"batch_norm_final\"))\n",
        "    model.add(layers.Activation(\"elu\", name=\"activation_final\"))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(1, activation='sigmoid', name=\"output\"))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Print summary\n",
        "    #model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# CNN\n",
        "def create_conv_model(learn_rate: float = 0.001, filters: int = 32, n_blocks: int = 0, kernel_size: int = 3):\n",
        "\n",
        "    # Define optimization settings\n",
        "    optimizer = Adam(lr=learn_rate)\n",
        "\n",
        "    # Initialize model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add embedding layer\n",
        "    model.add(layers.Embedding(vocab_size, embedding_dim,\n",
        "                               weights=[embedding_matrix],\n",
        "                               input_length=maxlen,\n",
        "                               trainable=False, \n",
        "                               name=\"embedding\"))\n",
        "\n",
        "    # Add convolution blocks\n",
        "    for blocks in range(n_blocks):\n",
        "        label = str(blocks)\n",
        "        model.add(layers.Conv1D(filters=filters,\n",
        "                                kernel_size=kernel_size,\n",
        "                                padding=\"same\",\n",
        "                                name=\"conv1D_\"+label))\n",
        "        model.add(layers.BatchNormalization(name=\"batch_norm_\"+label))\n",
        "        model.add(layers.Activation(\"relu\", name=\"activation_\"+label))\n",
        "        model.add(layers.MaxPool1D(pool_size=2, name\n",
        "=\"max_pool_\"+label))\n",
        "\n",
        "    # Final convolution block (w/o MaxPooling)\n",
        "    model.add(layers.Conv1D(filters=filters,\n",
        "                            kernel_size=kernel_size,\n",
        "                            padding=\"same\",\n",
        "                            name=\"conv1D_final_block\"))\n",
        "    model.add(layers.BatchNormalization(name=\"batch_norm_final_block\"))\n",
        "    model.add(layers.Activation(\"relu\", name=\"activation_final_block\"))\n",
        "    model.add(layers.Flatten(name=\"flatten\"))\n",
        "\n",
        "    # Dropout layer\n",
        "    model.add(layers.Dropout(0.5, name=\"dropout_flat_to_dense\"))\n",
        "    model.add(layers.Dense(32, name=\"dense\", \n",
        "                           activity_regularizer=L1L2(0.0, 1e-4)))\n",
        "    model.add(layers.BatchNormalization(name=\"batch_norm_dense\"))\n",
        "    model.add(layers.Activation(\"relu\", name=\"activation_dense\"))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(1, activation='sigmoid', name=\"output\"))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Print summary\n",
        "    #model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCM1RPSxec5c"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **1) Model Fitting with Keras/TensorFlow**\n",
        "\n",
        "Models (SavedModel and TF-lite) will be saved to the folder 'TensorFlow Models'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY9xce4selov"
      },
      "source": [
        "if not os.path.exists(\"TensorFlow Models\"):\n",
        "  os.mkdir(\"TensorFlow Models\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjfwOR73OaKI"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "### **1-D Convolutional Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOB9M7YoOcFA"
      },
      "source": [
        "# CNN model fitting\n",
        "cnn_model = create_conv_model(filters=8, learn_rate=1e-5, n_blocks=0, kernel_size=3)\n",
        "plot_model(cnn_model, to_file=\"TensorFlow Models/cnn_model.png\", show_shapes=True, show_layer_names=True)\n",
        "cnn_model.fit(X, y, batch_size=16, epochs=65, validation_split=0.5, shuffle=True)\n",
        "# Save as SavedModel\n",
        "cnn_save_model_file = \"TensorFlow Models/model_fit_conv_{:%Y%m%d_%H%M%S}\".format(datetime.now())\n",
        "cnn_model.save(filepath=cnn_save_model_file, overwrite=True, include_optimizer=True, save_format=None)\n",
        "# Convert to TF-Lite model and save\n",
        "cnn_converter = tf.lite.TFLiteConverter.from_keras_model(cnn_model)\n",
        "cnn_model_lite = cnn_converter.convert()\n",
        "open(\"TensorFlow Models/conv_sentiment_classifier.tflite\", \"wb\").write(cnn_model_conv_lite)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmcwk1JhTHGC"
      },
      "source": [
        "---\n",
        "### **Recurrent Neural Network**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwkBHmipTLso"
      },
      "source": [
        "# RNN\n",
        "rnn_model = create_lstm_model(units=4, learn_rate=1e-4, n_blocks=0)  # 25,505 trainable parameters\n",
        "plot_model(rnn_model, to_file=\"TensorFlow Models/rnn_model.png\", show_shapes=True, show_layer_names=True)\n",
        "rnn_model.fit(X, y, batch_size=16, epochs=5, validation_split=0.5, shuffle=True)\n",
        "# Save as SavedModel\n",
        "rnn_save_model_file = \"TensorFlow Models/model_fit_rnn_{:%Y%m%d_%H%M%S}\".format(datetime.now())\n",
        "rnn_model.save(filepath=rnn_save_model_file, overwrite=True, include_optimizer=True, save_format=None)\n",
        "# Convert to TF-Lite model and save\n",
        "rnn_converter = tf.lite.TFLiteConverter.from_keras_model(rnn_save_model_file)\n",
        "rnn_model_lite = rnn_converter.convert()\n",
        "open(\"TensorFlow Models/lstm_sentiment_classifier.tflite\", \"wb\").write(rnn_model_lite)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w1N3J53TSoz"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "### **Some quick, un-scientific tests of the models**\n",
        "\n",
        "Let's try the fitted models out on some example text. These sentences are meant to include a mix of clearly negative, neutral, and positive statements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVcKofC-TWKh"
      },
      "source": [
        "new_sentence = [\n",
        "    \"this place sucks so much. i hate it. i never want to go here ever again. please, listen to me when i tell you to avoid it like the plague.\",\n",
        "    \"this is the best movie i've ever seen so full of excitement and beautiful moments to cherish\",\n",
        "    \"it was ok, good, but not great. they should add more dinosaurs to make it better.\",\n",
        "    \"the movie was pretty good and i liked most of it, but the acting was could use some work\",\n",
        "    \"this is the worst product i've ever purchased. it broke within hours of use.\",\n",
        "    \"new research reveals the secret to being the cutest marine animal that ever existed.\",\n",
        "    \"global carbon emissions are down over 80 percent as climate improves for millions\",\n",
        "    \"congress passes legislation to protect endangered sea turtles.\",\n",
        "    \"reading this book was life affirming and now i have the confidence to express my best work. great job. this is the most awesome thing ever.\",\n",
        "    \"the lemon potatoes were disgusting and i had a bad time. overall, this place is gross. don't ever go here if you can help it.\"]\n",
        "#new_sentence = [\"This text sentiment analyzer could be used when practicing for a presentation or drafting a writing project. It predicts the sentinment of example text using deep learning models that were trained on the IMDB movie reviews data set. See below for more information about how the models were created.\"]\n",
        "new_sentence_processed = pre_process_sentence(new_sentence)\n",
        "new_sentence_tokenized = tokenizer.texts_to_sequences(new_sentence_processed)\n",
        "X_new = pad_sequences(new_sentence_tokenized, padding='post', maxlen=250)\n",
        "\n",
        "# CNN predictions for new data\n",
        "#y_new = cnn_model.predict(X_new)\n",
        "#for text, sentiment in zip(new_sentence, y_new):\n",
        "#    print(f\"{text}: {np.round(sentiment,3)}\")\n",
        "\n",
        "# RNN predictions for new data\n",
        "y_new = rnn_model.predict(X_new)\n",
        "for text, sentiment in zip(new_sentence, y_new):\n",
        "    print(f\"{text}: {np.round(sentiment,3)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUm5if0TdpGK"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **2) Save Fitted Models to Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "912IS7gBdoyE"
      },
      "source": [
        "import shutil\n",
        "shutil.make_archive('TensorFlow Models', 'zip', 'TensorFlow Models')\n",
        "!gsutil cp -r \"TensorFlow Models.zip\" \"/content/drive/My Drive/CYT\"\n",
        "shutil.make_archive('tokenizer', 'zip', 'tokenizer')\n",
        "!gsutil cp -r \"tokenizer.zip\" \"/content/drive/My Drive/CYT\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5axepN8kvsn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **LICENSE**\n",
        "\n",
        "*MIT License*\n",
        "\n",
        "*Copyright (c) 2020 Eric Van Cleave*\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    }
  ]
}